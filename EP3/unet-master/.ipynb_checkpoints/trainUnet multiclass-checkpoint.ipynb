{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from data import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your Unet with membrane data\n",
    "class data is in folder {class}/, it is a binary classification task.\n",
    "\n",
    "https://github.com/zhixuhao/unet\n",
    "\n",
    "The input shape of image and mask are the same :(batch_size,rows,cols,channel = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_generator(classe):\n",
    "    path = os.path.join('data/',classe,'train')\n",
    "    print('Iniciando classe: ',classe)\n",
    "    data_gen_args = dict(rotation_range=0.2,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=0.05,\n",
    "                    zoom_range=0.05,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "    myGene = trainGenerator(6,os.path.join('data/',classe,'train'),'image','label',data_gen_args,save_to_dir = None)\n",
    "    model = unet()\n",
    "    model_checkpoint = ModelCheckpoint('unet_'+classe+'.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "    model.fit_generator(myGene,steps_per_epoch=2000,epochs=3,callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [#'caneca','caneta',\n",
    "    #'chave',\n",
    "    #'creme_dental',\n",
    "    #'faca',\n",
    "    'fio_dental',\n",
    "    #'meia',\n",
    "    'perfume',\n",
    "    #'pilha',\n",
    "    #'shampoo'\n",
    "]\n",
    "# caneca, caneta, faca = 2, 1000, 4\n",
    "# creme_dental, meia = 4,1000,4\n",
    "# chave, pilha: 5,1000,5\n",
    "# shampoo: 6,1000,5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando classe:  fio_dental\n",
      "Found 85 images belonging to 1 classes.\n",
      "Found 85 images belonging to 1 classes.\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000026601443438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000026601443438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "   2/2000 [..............................] - ETA: 18:13 - loss: 0.6918 - accuracy: 0.5063WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2153s vs `on_train_batch_end` time: 0.4310s). Check your callbacks.\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9772\n",
      "Epoch 00001: loss improved from inf to 0.06757, saving model to unet_fio_dental.hdf5\n",
      "2000/2000 [==============================] - 1263s 632ms/step - loss: 0.0676 - accuracy: 0.9772\n",
      "Epoch 2/3\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9975\n",
      "Epoch 00002: loss improved from 0.06757 to 0.00634, saving model to unet_fio_dental.hdf5\n",
      "2000/2000 [==============================] - 1265s 633ms/step - loss: 0.0063 - accuracy: 0.9975\n",
      "Epoch 3/3\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 0.9980\n",
      "Epoch 00003: loss improved from 0.00634 to 0.00502, saving model to unet_fio_dental.hdf5\n",
      "2000/2000 [==============================] - 1268s 634ms/step - loss: 0.0050 - accuracy: 0.9980\n",
      "Iniciando classe:  perfume\n",
      "Found 90 images belonging to 1 classes.\n",
      "Found 90 images belonging to 1 classes.\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000026613F789D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000026613F789D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "   2/2000 [..............................] - ETA: 19:25 - loss: 0.7349 - accuracy: 0.7470WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2260s vs `on_train_batch_end` time: 0.4691s). Check your callbacks.\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9732\n",
      "Epoch 00001: loss improved from inf to 0.06935, saving model to unet_perfume.hdf5\n",
      "2000/2000 [==============================] - 1373s 687ms/step - loss: 0.0693 - accuracy: 0.9732\n",
      "Epoch 2/3\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.0133 - accuracy: 0.9947\n",
      "Epoch 00002: loss improved from 0.06935 to 0.01327, saving model to unet_perfume.hdf5\n",
      "2000/2000 [==============================] - 1350s 675ms/step - loss: 0.0133 - accuracy: 0.9947\n",
      "Epoch 3/3\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9968\n",
      "Epoch 00003: loss improved from 0.01327 to 0.00784, saving model to unet_perfume.hdf5\n",
      "2000/2000 [==============================] - 1344s 672ms/step - loss: 0.0078 - accuracy: 0.9968\n"
     ]
    }
   ],
   "source": [
    "for classe in classes:\n",
    "    model_generator(classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgs_train,imgs_mask_train = geneTrainNpy(\"data/membrane/train/aug/\",\"data/membrane/train/aug/\")\n",
    "#model.fit(imgs_train, imgs_mask_train, batch_size=2, nb_epoch=10, verbose=1,validation_split=0.2, shuffle=True, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test your model and save predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results(classe):\n",
    "    print(classe)\n",
    "    print(\"------------------------------------------------------------------------------------\")\n",
    "    test_path = os.path.join(\"data/\",classe,\"test\")\n",
    "    results_path = os.path.join(\"data/\",classe,\"results\")\n",
    "    files_count = len(os.listdir(test_path))\n",
    "    testGene = testGenerator(test_path,num_image=files_count)\n",
    "    model = unet()\n",
    "    model.load_weights(\"unet_\"+ classe+\".hdf5\")\n",
    "    results = model.predict_generator(testGene,files_count,verbose=1)\n",
    "    saveResult(results_path,results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fio_dental\n",
      "------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000026613F78AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000026613F78AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "  2/540 [..............................] - ETA: 13sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_predict_batch_end` time: 0.0450s). Check your callbacks.\n",
      "540/540 [==============================] - 23s 43ms/step\n",
      "perfume\n",
      "------------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002660DE584C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002660DE584C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "  2/540 [..............................] - ETA: 15sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0006s vs `on_predict_batch_end` time: 0.0456s). Check your callbacks.\n",
      "540/540 [==============================] - 23s 43ms/step\n"
     ]
    }
   ],
   "source": [
    "for classe in classes:\n",
    "    generate_results(classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
